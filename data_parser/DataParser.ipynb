{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudPerformance - Data Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DOWNLOAD DATA HERE](https://mega.nz/folder/lMFTkQLK#uFvAUKTCT2jDfO0mgs5ZPw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"gcp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"egi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"local\"\n",
    "source_file = provider + \".json\"\n",
    "config_file = \"config_data_parser_\" + provider + \".yml\"\n",
    "ignore_vms_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = \"remote\"\n",
    "# bin_database_url = \"http://bin_url/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parsing and analysis\n",
    "- the suite of benchmarks is repeated at every interval of time\n",
    "- a (micro)benchmark can be repeated multiple times\n",
    "- a (micro)benchmark can have multiple setup\n",
    "- the benchmark time is taken when the suite of benchmark is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import csv\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pickle\n",
    "import hashlib\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot format\n",
    "my_template = dict(\n",
    "    layout=go.Layout(title_font=dict(family=\"Helvetica\", size=32),\n",
    "                    font_family=\"Helvetica\",\n",
    "                    font_size=24)\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "cmap = plt.get_cmap('jet_r')\n",
    "font = {'family' : 'Helvetica',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 24}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIRS = [\"output/execution/html/\", \"output/execution/img/\", \"output/time/html/\", \"output/time/img/\", \"output/mean/html/\", \"output/mean/img/\", \"output/delta/html/\", \"output/delta/img/\", \"output/relations/img\", \"pickle/\"]\n",
    "for output_dir in OUTPUT_DIRS:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "# set date formatter\n",
    "date_formatter = DateFormatter('%H:%M')        \n",
    "\n",
    "# graphs colors\n",
    "instance_colors = {\"a1.large - eu-central-1a\": \"#f29e4c\",\n",
    "                  \"a1.large - eu-central-1b\": \"#f29e4c\",\n",
    "                  \"a1.xlarge - eu-central-1b\": \"#e36414\",\n",
    "                  \"m5.large - eu-central-1a\": \"#b9e769\",\n",
    "                  \"m5.large - eu-central-1b\": \"#b9e769\",\n",
    "                  \"m5.xlarge - eu-central-1b\": \"#3fa34d\",\n",
    "                  \"A2-1 - eu\": \"#9bc1bc\",\n",
    "                  \"A2-2 - eu\": \"#9bc1bc\",\n",
    "                  \"A4-1 - eu\": \"#5ca4a9\",\n",
    "                  \"B2MS-1 - eu\": \"#b298dc\",\n",
    "                  \"B2MS-2 - eu\": \"#b298dc\",\n",
    "                  \"B4MS-1 - eu\": \"#a663cc\",\n",
    "                  \"e2-t1-1 - euw3a\": \"#b8b8b8\",\n",
    "                  \"e2-t1-2 - euw3b\": \"#b8b8b8\",\n",
    "                  \"e2-t2-1 - euw3a\": \"#878787\",\n",
    "                  \"n1-t1-1 - euw3a\": \"#efc050\",\n",
    "                  \"n1-t1-2 - euw3b\": \"#efc050\",\n",
    "                  \"n1-t2-1 - euw3a\": \"#b88711\",\n",
    "                  \"EGI_T1-1\": \"#f72585\",\n",
    "                  \"EGI_T1-2\": \"#f72585\",\n",
    "                  \"EGI_T2-1\": \"#4cc9f0\",\n",
    "                  \"EGI_T3-1\": \"#ffe8d6\",\n",
    "                  \"EGI_T3-2\": \"#ffe8d6\",\n",
    "                  \"EGI_T4-1\": \"#fdc500\"}\n",
    "\n",
    "def get_instance_color(s):\n",
    "    if s in instance_colors:\n",
    "        return instance_colors[s]\n",
    "    else:\n",
    "        return \"#a89984\"\n",
    "\n",
    "def get_valid_filename(s):\n",
    "    s = str(s).strip().replace(' ', '_')\n",
    "    return re.sub(r'(?u)[^-\\w.]', '', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def plot_dates(x, y, label=None, suptitle=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    if suptitle:\n",
    "        fig.suptitle(suptitle)\n",
    "    ax.plot(x, y, label=label)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def plotly_dates(x, y, label=None, suptitle=None):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=label))\n",
    "    fig.update_layout(autosize=False)\n",
    "    fig.update_yaxes(automargin=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers Definition\n",
    "Extract useful data from benchmarks outputs using regexps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_sysbench(raw_data, print_errors=True):\n",
    "    sys_res = []\n",
    "    cpu_tests = [(r\"events per second:\\s*([0-9.]*)\", \"cpu: events per second\", \"events/s\"),\n",
    "                (r\"avg:\\s*([0-9.]*)\", \"cpu: avg latency\", \"ms\")]\n",
    "    fileio_tests = [(r\"reads\\/s:\\s*([0-9.]*)\", \"fileio: fileop - read\", \"reads/s\"),\n",
    "                    (r\"writes\\/s:\\s*([0-9.]*)\", \"fileio: fileop - write\", \"writes/s\"),\n",
    "                    (r\"fsyncs\\/s:\\s*([0-9.]*)\", \"fileio: fileop - fsync\", \"fsyncs/s\"),\n",
    "                    (r\"read, MiB\\/s:\\s*([0-9.]*)\", \"fileio: throughput - read\", \"MiB/s\"),\n",
    "                    (r\"written, MiB\\/s:\\s*([0-9.]*)\", \"fileio: throughput - write\", \"MiB/s\"),\n",
    "                    (r\"avg:\\s*([0-9.]*)\", \"fileio: avg latency\", \"ms\")]\n",
    "    memory_tests = [(r\"([0-9.,]*) per second\", \"memory: totalops per second\", \"ops/s\"),\n",
    "                    (r\"([0-9.,]*) MiB\\/sec\", \"memory: speed\", \"MiB/s\"),\n",
    "                    (r\"avg:\\s*([0-9.]*)\", \"memory: avg latency\", \"ms\")]\n",
    "    threads_tests = [(r\"avg:\\s*([0-9.]*)\", \"threads: avg latency\", \"ms\")]\n",
    "    for benchmark in [(\"cpu\", cpu_tests), (\"fileio\", fileio_tests),\n",
    "                      (\"memory\", memory_tests), (\"threads\", threads_tests)]:\n",
    "        r_data = raw_data[benchmark[0]]\n",
    "        tests = benchmark[1]\n",
    "        for test in tests:\n",
    "            match = re.findall(test[0], r_data, re.MULTILINE)\n",
    "            res = float(match[0].replace(\",\", \".\"))\n",
    "            name = test[1]\n",
    "            unit = test[2]\n",
    "            sys_res.append({\"unit\": unit, \"value\": res, \"name\": name})\n",
    "    return sys_res\n",
    "\n",
    "def parser_simplecpu(raw_data, print_errors=True):\n",
    "    return [{\"unit\": \"s\", \"value\": raw_data, \"name\": \"mean duration\"}]\n",
    "\n",
    "def parser_download(raw_data, print_errors=True):\n",
    "    # extract size and time\n",
    "    regex = r\"Downloaded\\s([0-9]*)[a-z\\s]*([0-9.,]*)\"\n",
    "    match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "    size = float(match[0][0])/1000/1000\n",
    "    time = float(match[0][1].replace(\",\", \".\"))\n",
    "    return [{\"unit\": \"MB/s\", \"value\": round(size/time, 2), \"name\": \"speed\"}]\n",
    "\n",
    "def parser_dd(raw_data, print_errors=True):\n",
    "    # extract speed\n",
    "    regex = r\"s,\\s([0-9.,]*)\\s[a-zA-Z]*\\/s\"\n",
    "    match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "    speed = float(str(match[0]).replace(\",\", \".\"))\n",
    "    return [{\"unit\": \"MB/s\", \"value\": speed, \"name\": \"speed\"}]\n",
    "\n",
    "def parser_web_benchmark(raw_data, print_errors=True):\n",
    "    regex = r\"Requests/sec:\\s*([0-9.]*)\"\n",
    "    match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "    speed = float(str(match[0]).replace(\",\", \".\"))\n",
    "    return [{\"unit\": \"req/s\", \"value\": speed, \"name\": \"requests per second\"}]\n",
    "\n",
    "def parser_nench(raw_data, print_errors=True):\n",
    "    # cpu\n",
    "    try:\n",
    "        cpu_res = []\n",
    "        for cpu_test in [\"SHA256\", \"bzip2\", \"AES\"]:\n",
    "            regex = re.escape(cpu_test) + \".*\\n\\s*([0-9,.]*) ([a-z]*)\"\n",
    "            match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "            res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "            unit = str(match[0][1])\n",
    "            cpu_res.append({\"unit\": unit, \"value\": res, \"name\": \"cpu - \" + cpu_test})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - cpu: \" + str(e))\n",
    "    # ioping\n",
    "    try:\n",
    "        ioping_res = []\n",
    "        seekrate = (r\"min\\/avg\\/max\\/mdev = [0-9a-z.\\s]*\\/ ([0-9.]*) ([a-z]*)\", \"ioping: avg seek rate\")\n",
    "        srs = (r\"generated.*iops. ([0-9.]*) (.*)\", \"ioping: sequential read speed\")\n",
    "        for ioping in [seekrate, srs]:\n",
    "            regex = ioping[0]\n",
    "            match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "            res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "            unit = str(match[0][1])\n",
    "            ioping_res.append({\"unit\": unit, \"value\": res, \"name\": ioping[1]})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - ioping: \" + str(e))\n",
    "        pass\n",
    "    # dd\n",
    "    try:\n",
    "        dd_res = []\n",
    "        regex = r\"average:\\s*([0-9.]*) ([a-zA-Z\\/]*)$\"\n",
    "        match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "        res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "        unit = str(match[0][1])\n",
    "        dd_res.append({\"unit\": unit, \"value\": res, \"name\": \"dd: avg sequential write speed\"})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - dd: \" + str(e))\n",
    "    # speed test\n",
    "    try:\n",
    "        speed_res = []\n",
    "        for speed_test in [\"Cachefly CDN\", \"Leaseweb (NL)\", \"Softlayer DAL (US)\", \"Online.net (FR)\", \"OVH BHS (CA)\"]:\n",
    "            regex = re.escape(speed_test) + \":\\s*([0-9.]*) ([a-zA-Z\\/]*)\"\n",
    "            match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "            res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "            unit = str(match[0][1])\n",
    "            speed_res.append({\"unit\": unit, \"value\": res, \"name\": \"net speed - \" + speed_test})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - speed test: \" + str(e))\n",
    "    return cpu_res + ioping_res + dd_res + speed_res\n",
    "\n",
    "def parser_aibenchmark(raw_data, print_errors=True):\n",
    "    ai_res = []\n",
    "    if raw_data[\"ai_score\"]:\n",
    "        ai_res.append({\"unit\": \"score\", \"value\": raw_data[\"ai_score\"], \"name\": \"ai score\"})\n",
    "    if raw_data[\"inference_score\"]:\n",
    "        ai_res.append({\"unit\": \"score\", \"value\": raw_data[\"inference_score\"], \"name\": \"inference score\"})\n",
    "    if raw_data[\"training_score\"]:\n",
    "        ai_res.append({\"unit\": \"score\", \"value\": raw_data[\"training_score\"], \"name\": \"training score\"})\n",
    "    return ai_res\n",
    "\n",
    "def htr(unit):\n",
    "    if unit == \"s\" or unit == \"seconds\" or unit ==\"ms\" or unit == \"us\":\n",
    "        return \"LIB\"\n",
    "    return \"HIB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply parsers and extract data\n",
    "For each benchmark extract a list of values (a benchmark can provide multiple extracted values, such as sysbench):\n",
    "- extracted.name: is the name of the benchmark value extracted, e.g. cpu - SHA256\n",
    "- extracted.value: is the extracted value, e.g. 3.279\n",
    "- extracted.unit: extracted unit, e.g. seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "benchmark_extractors = {\"download\": parser_download,\n",
    "                        \"dd\": parser_dd,\n",
    "                        \"web-benchmark\": parser_web_benchmark,\n",
    "                        \"nench-benchmark\": parser_nench,\n",
    "                        \"sys-benchmark\": parser_sysbench,\n",
    "                        \"simple-cpu\": parser_simplecpu,\n",
    "                        \"ai-benchmark\": parser_aibenchmark}\n",
    "\n",
    "\n",
    "# get configuration\n",
    "with open(config_file, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "if source == \"local\":\n",
    "    with open(source_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "elif source == \"remote\":\n",
    "    # get data from the database\n",
    "    url = bin_database_url + \"/all\"\n",
    "    # print(\"getting data from: \" + url)\n",
    "    response = requests.get(url=url)\n",
    "    data = response.json()\n",
    "\n",
    "print(\"total VMs: %d\\n\\n\" % len(data[\"bins\"]))\n",
    "\n",
    "durations_all = []\n",
    "benchmarks_all = 0\n",
    "# iter vms\n",
    "for bin in data[\"bins\"]:\n",
    "    pars_err = 0\n",
    "    bench_err = 0\n",
    "    id = bin[\"id\"]\n",
    "    if id in ignore_vms_list:\n",
    "        continue\n",
    "    updated = bin[\"updated\"]\n",
    "    benchmarks = len(bin[\"values\"])\n",
    "    server = bin[\"values\"][0][\"server\"]\n",
    "    \n",
    "    print(id)\n",
    "    print(json.dumps(server, indent=2))\n",
    "    \n",
    "    # iters bench runs\n",
    "    durations = []\n",
    "    times = []\n",
    "    results = {}\n",
    "    for value in bin[\"values\"]:\n",
    "        durations_all.append(value[\"duration\"])\n",
    "        durations.append(value[\"duration\"])\n",
    "        time = datetime.datetime.strptime(value[\"time\"], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        times.append(time)\n",
    "        \n",
    "        # iter benchmarks\n",
    "        for benchmark in value[\"benchmarks\"]:\n",
    "            if benchmark[\"name\"] in benchmark_extractors.keys():\n",
    "                if benchmark[\"result\"][\"retcode\"] != 0:\n",
    "                    # error during the execution of the benchmark\n",
    "                    #print(\"Error benchmark: \" + benchmark[\"name\"] + str(benchmark[\"setup\"]) + \" @ \" + str(time))\n",
    "                    bench_err +=1\n",
    "                else:\n",
    "                    try:\n",
    "                        benchmark[\"extracted\"] = benchmark_extractors[benchmark[\"name\"]](benchmark[\"result\"][\"output\"], print_errors=False)\n",
    "                        benchmarks_all += 1\n",
    "                    except Exception as e:\n",
    "                        # error during the parsing of the output\n",
    "                        print(\"Error extracting: \" + benchmark[\"name\"] + \" @ \" + str(time) + \"\\n\" + str(e))\n",
    "                        pars_err += 1\n",
    "    \n",
    "    print(\"Benchmark errors: \" + str(bench_err))\n",
    "    print(\"Parsing errors: \" + str(pars_err))\n",
    "        \n",
    "    print(\"total runs: %d\\nupdated: %s\\nduration (min, avg, max, std): %.2f m, %.2f m, %.2f m, %.2f m\"\n",
    "          % (benchmarks, updated, min(durations)/60, stat.mean(durations)/60, max(durations)/60, stat.stdev(durations)/60))\n",
    "    \n",
    "    fig = go.Figure([go.Scatter(x=times, y=durations)])\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        #title=provider + \" / \" + cfg[\"vms\"][id.split(\"_\", 1)[1]][\"name\"],\n",
    "        title_x=0.5,\n",
    "        xaxis = dict(\n",
    "            #title = 'time',\n",
    "            showticklabels=False),\n",
    "        #yaxis_title=\"duration\",\n",
    "        template=my_template\n",
    "    )\n",
    "    fig.write_image(\"output/execution/img/\" + id + \".png\", scale=2)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.write_html(\"output/execution/html/\" + id + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.show()\n",
    "\n",
    "print(\"OVERALL benchmarks: %d\\nduration (%d) (min, avg, max, std): %.2f m, %.2f m, %.2f m, %.2f m\"\n",
    "          % (benchmarks_all, len(durations_all), min(durations_all)/60, stat.mean(durations_all)/60, max(durations_all)/60, stat.stdev(durations_all)/60))\n",
    "# print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "- use Pandas to analyze data\n",
    "- preparare the data for Pandas: build a list where each entry is a benchmark value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "tests_uni = {}\n",
    "tests_counter = {}\n",
    "# iter vms\n",
    "for bin in data[\"bins\"]:\n",
    "    id = bin[\"id\"]\n",
    "    if id in ignore_vms_list:\n",
    "        continue\n",
    "    updated = bin[\"updated\"]\n",
    "    server = bin[\"values\"][0][\"server\"]\n",
    "    \n",
    "    # iters bench run\n",
    "    for value in bin[\"values\"]:\n",
    "        duration = value[\"duration\"]\n",
    "        time = value[\"time\"]\n",
    "        \n",
    "        # iter benchmarks\n",
    "        for benchmark in value[\"benchmarks\"]:\n",
    "            if \"extracted\" not in benchmark:\n",
    "                continue\n",
    "            extracted = benchmark[\"extracted\"]\n",
    "            \n",
    "            for ext in extracted:\n",
    "                test_id = hashlib.sha1(str(str(benchmark[\"name\"] + str(benchmark[\"setup\"]) + ext[\"name\"])).encode()).hexdigest()\n",
    "                row = {\"id\": cfg[\"vms\"][id.split(\"_\", 1)[1]][\"name\"],\n",
    "                       \"duration\": duration,\n",
    "                       \"time\": pd.to_datetime(time, format='%Y-%m-%d %H:%M:%S.%f'),\n",
    "                       \"value\": ext[\"value\"],\n",
    "                       \"test_id\": test_id}\n",
    "                \n",
    "                tests_uni[test_id] = {\"bench_name\": benchmark[\"name\"],\n",
    "                                      \"setup\": str(benchmark[\"setup\"]),\n",
    "                                      \"extracted_name\": ext[\"name\"],\n",
    "                                      \"extracted_unit\": ext[\"unit\"],\n",
    "                                      \"htr\": htr(ext[\"unit\"]) }\n",
    "                \n",
    "                if test_id not in tests_counter:\n",
    "                    tests_counter[test_id] = 1\n",
    "                else:\n",
    "                    tests_counter[test_id] += 1\n",
    "                \n",
    "                rows.append(row)\n",
    "                \n",
    "# save tests\n",
    "pickle.dump(tests_uni, open(\"pickle/tests_uni.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(tests_uni, indent=2))\n",
    "\n",
    "for test in tests_uni:\n",
    "    tests_uni[test][\"bench_name\"]\n",
    "\n",
    "bench_list = []\n",
    "print(\"\\nBenchmark list (\" + str(len(tests_uni)) + \"):\")\n",
    "for test in tests_uni:\n",
    "    bench_list.append({\n",
    "        \"id\": test, \n",
    "        \"bench_name\": tests_uni[test][\"bench_name\"],\n",
    "        \"extracted_name\": tests_uni[test][\"extracted_name\"],        \n",
    "    })\n",
    "    print(tests_uni[test][\"bench_name\"], tests_uni[test][\"extracted_name\"], tests_uni[test][\"extracted_unit\"], tests_uni[test][\"htr\"])\n",
    "\n",
    "print(\"\\nBench list:\")\n",
    "print(json.dumps(bench_list, indent=2))\n",
    "    \n",
    "print(\"\\n\\nCounters:\")\n",
    "for test in tests_counter:\n",
    "    print(tests_uni[test][\"bench_name\"],  tests_uni[test][\"extracted_name\"], tests_counter[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(rows))\n",
    "# normalize the data\n",
    "pd.set_option('display.max_rows', 20)\n",
    "df = pd.json_normalize(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the extracted values for each benchmark with all the machines in a single graph\n",
    "1. compute the mean for each benchmark for each machine grouping by: id, test_id, time\n",
    "2. reset_index() to obtain a list of tuples\n",
    "3. groupby bench_name, setup, extracted.name\n",
    "4. plot for each group, grouping by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "gb = df.groupby(['id', 'test_id', 'time']).agg(\n",
    "    {'value': ['mean']}).reset_index()\n",
    "gb.columns = gb.columns.droplevel(1)\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "gbb = gb.groupby(['test_id'])\n",
    "# save gbb\n",
    "pickle.dump(gbb, open(\"pickle/\" + provider + \".p\", \"wb\"))\n",
    "gbb.first() # print the first entry (can be many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Result Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(provider, bench_name, extracted_name, key):\n",
    "    return provider + \"_\" + get_valid_filename(bench_name + \"_\" + extracted_name) + \"_\" + str(key)[0:5]\n",
    "def get_trace_name(name):\n",
    "    if provider == \"aws\":\n",
    "        #return name.replace(\"-central\", \"\")\n",
    "        return name[:name.find(\" -\")]+\"-\"+name[-1:]\n",
    "    elif provider == \"azure\":\n",
    "        return name\n",
    "    elif provider == \"gcp\":\n",
    "        return name\n",
    "    elif provider == \"egi\":\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, grp in gbb:\n",
    "    print(\"Bench Id:\", key)\n",
    "    print(tests_uni[key])\n",
    "    print(\"\\n\")\n",
    "    fig = go.Figure()\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        times = grp2[\"time\"]\n",
    "        values = grp2[\"value\"]\n",
    "        fig.add_trace(go.Scatter(x=times, y=values, mode='lines', name=get_trace_name(key2)))\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        #title=provider + \" / \" + tests_uni[key][\"bench_name\"] + \" - \" + tests_uni[key][\"extracted_name\"] + \" (\" + tests_uni[key][\"extracted_unit\"] + \")\",\n",
    "        title_x=0.5,\n",
    "        xaxis = dict(\n",
    "            #title = 'time',\n",
    "            showticklabels=False),\n",
    "        #yaxis_title=\"result\",\n",
    "        template=my_template\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(\"output/time/html/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.write_image(\"output/time/img/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Result Bar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, grp in gbb:\n",
    "    print(\"Bench Id:\", key)\n",
    "    print(tests_uni[key])\n",
    "    print(\"\\n\")\n",
    "    fig = go.Figure()\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        print(\"%s - \\tAVG: %.2f, STD: %.2f, MIN: %.2f, MAX: %.2f\" % (key2, grp2.value.mean(), grp2.value.std(), grp2.value.min(), grp2.value.max()))\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=get_trace_name(key2),\n",
    "            x=[get_trace_name(key2)], y=[grp2.value.mean()],\n",
    "            error_y=dict(type='data', array=[grp2.value.std()]),\n",
    "            marker_color=get_instance_color(key2)\n",
    "        ))\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        #title=provider + \" / \" + tests_uni[key][\"bench_name\"] + \" - \" + tests_uni[key][\"extracted_name\"] + \" (\" + tests_uni[key][\"extracted_unit\"] + \")\",\n",
    "        title_x=0.5,\n",
    "        #xaxis_title=\"instance\",\n",
    "        #yaxis_title=\"result\",\n",
    "        showlegend=False,\n",
    "        #margin=dict(l=50, r=80, t=50, b=100),\n",
    "        template=my_template\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(\"output/mean/html/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.write_image(\"output/mean/img/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Result Delta Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, grp in gbb:\n",
    "    print(\"Bench Id:\", key)\n",
    "    print(tests_uni[key])\n",
    "    print(\"\\n\")\n",
    "    fig = go.Figure()\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        times = grp2[\"time\"]\n",
    "        values = [v/grp2.value.mean()*100 for v in grp2[\"value\"]]\n",
    "        fig.add_trace(go.Scatter(x=times, y=values, mode='lines', name=get_trace_name(key2)))\n",
    "        print(\"%s - \\tAVG: %.2f, STD: %.2f, MIN: %.2f, MAX: %.2f\" % (key2, stat.mean(values), stat.stdev(values), min(values), max(values)))\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_xaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        title=provider + \" / \" + tests_uni[key][\"bench_name\"] + \" - \" + tests_uni[key][\"extracted_name\"] + \" (\" + tests_uni[key][\"extracted_unit\"] + \")\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"time\",\n",
    "        yaxis_title=\"Î”\",\n",
    "        template=my_template\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(\"output/delta/html/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.write_image(\"output/delta/img/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance relations between different resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alg1\n",
    "1. compute the gradient\n",
    "2. take the n_mm-max from the abs results and take the relative date time where the maxes occur\n",
    "3. compare the n-max for each couple of benchmarks computing the intersection\n",
    "4. if the number of intersection is greater than a threshold the benchmarks behaviour may be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "n_mm = 100\n",
    "# compute the n min max\n",
    "mm_data = {}\n",
    "for key, grp in gbb:\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        times = grp2[\"time\"].tolist()\n",
    "        deltas = [v/grp2.value.mean()*100 for v in grp2[\"value\"]]\n",
    "        gradient = np.gradient(deltas)\n",
    "\n",
    "        indexed = list(enumerate(abs(gradient)))\n",
    "        max_g = sorted(indexed, key=operator.itemgetter(1))[-n_mm:]\n",
    "        max_g_x = list(reversed([times[i] for i, v in max_g]))\n",
    "        max_g_y = list(reversed([gradient[i] for i, v in max_g]))\n",
    "        \n",
    "        if key2 not in mm_data:\n",
    "            mm_data[key2] = {}\n",
    "        mm_data[key2][key] = {\"t\": times,\n",
    "                              \"d\": deltas,\n",
    "                              \"g\": gradient,\n",
    "                              \"max_g_x\": max_g_x,\n",
    "                              \"max_g_y\": max_g_y}\n",
    "        print(key2, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3\n",
    "\n",
    "th = 0.6\n",
    "for m in mm_data:\n",
    "    print(\"------\", m)\n",
    "    i = j = 0\n",
    "    for b1 in mm_data[m]:\n",
    "        i += 1\n",
    "        j = 0\n",
    "        for b2 in mm_data[m]:\n",
    "            if b1 == b2 or j < i:\n",
    "                j += 1\n",
    "            else:\n",
    "                inters = intersection(mm_data[m][b1][\"max_g_x\"], mm_data[m][b2][\"max_g_x\"])\n",
    "                if len(inters) == n_mm:\n",
    "                    print(\"Full intersection\", tests_uni[b2][\"bench_name\"], tests_uni[b2][\"extracted_name\"], \"//\", tests_uni[b1][\"bench_name\"], tests_uni[b1][\"extracted_name\"])\n",
    "                elif len(inters) >= n_mm*th:\n",
    "                    print(colored(b2 + \" \" + b1, \"green\"))\n",
    "                    print(colored(tests_uni[b2][\"bench_name\"] + \" \" + tests_uni[b2][\"extracted_name\"] + \" // \" + tests_uni[b1][\"bench_name\"] + \" \" + tests_uni[b1][\"extracted_name\"], 'green'))\n",
    "                    print(colored(\"intersections: \" + str(len(inters)) + \" - \" + str(len(inters)/n_mm*100) + \"%\", \"green\"))\n",
    "                    fig = go.Figure()\n",
    "                    fig.add_trace(go.Scatter(x=mm_data[m][b1][\"t\"],\n",
    "                                             y=mm_data[m][b1][\"g\"],\n",
    "                                             mode='lines',\n",
    "                                             name=\"grad \" + tests_uni[b1][\"bench_name\"]))\n",
    "                    fig.add_trace(go.Scatter(x=mm_data[m][b1][\"max_g_x\"],\n",
    "                                             y=mm_data[m][b1][\"max_g_y\"],\n",
    "                                             mode='markers',\n",
    "                                             name=tests_uni[b1][\"bench_name\"]))\n",
    "                    fig.add_trace(go.Scatter(x=mm_data[m][b2][\"t\"],\n",
    "                                             y=mm_data[m][b2][\"g\"],\n",
    "                                             mode='lines',\n",
    "                                             name=\"grad \" + tests_uni[b2][\"bench_name\"]))\n",
    "                    fig.add_trace(go.Scatter(x=mm_data[m][b2][\"max_g_x\"],\n",
    "                                             y=mm_data[m][b2][\"max_g_y\"],\n",
    "                                             mode='markers',\n",
    "                                             name=tests_uni[b2][\"bench_name\"]))\n",
    "                    fig.update_layout(autosize=True)\n",
    "                    fig.update_yaxes(automargin=True)\n",
    "                    fig.update_xaxes(automargin=True)\n",
    "                    fig.update_layout(\n",
    "                        xaxis = dict(\n",
    "                        #title = 'time',\n",
    "                        showticklabels=False),\n",
    "                        showlegend=False,\n",
    "                        template=my_template\n",
    "                    )\n",
    "                    fig.show()\n",
    "                    fig.write_image(\"output/relations/img/\" + get_filename(provider, tests_uni[b2][\"bench_name\"], tests_uni[b2][\"extracted_name\"], key) + \"_\" + get_filename(provider, tests_uni[b1][\"bench_name\"], tests_uni[b1][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "\n",
    "    print(\"_________________________________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
