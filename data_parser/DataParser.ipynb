{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudPerformance - Data Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DOWNLOAD DATA HERE](https://mega.nz/folder/lMFTkQLK#uFvAUKTCT2jDfO0mgs5ZPw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"aws\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = \"gcp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"local\"\n",
    "source_file = provider + \".json\"\n",
    "config_file = \"config_data_parser_\" + provider + \".yml\"\n",
    "ignore_vms_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parsing and analysis\n",
    "- the suite of benchmarks is repeated at every interval of time\n",
    "- a (micro)benchmark can be repeated multiple times\n",
    "- a (micro)benchmark can have multiple setup\n",
    "- the benchmark time is taken when the suite of benchmark is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import csv\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pickle\n",
    "import hashlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot format\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "cmap = plt.get_cmap('jet_r')\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "OUTPUT_DIRS = [\"output/execution/html/\", \"output/execution/img/\", \"output/time/html/\", \"output/time/img/\", \"output/bar/html/\", \"output/bar/img/\", \"output/delta/html/\", \"output/delta/img/\", \"pickle/\"]\n",
    "for output_dir in OUTPUT_DIRS:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "# set date formatter\n",
    "date_formatter = DateFormatter('%H:%M')        \n",
    "\n",
    "def get_valid_filename(s):\n",
    "    s = str(s).strip().replace(' ', '_')\n",
    "    return re.sub(r'(?u)[^-\\w.]', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def plot_dates(x, y, label=None, suptitle=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    if suptitle:\n",
    "        fig.suptitle(suptitle)\n",
    "    ax.plot(x, y, label=label)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    fig.autofmt_xdate()\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def plotly_dates(x, y, label=None, suptitle=None):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=label))\n",
    "    fig.update_layout(autosize=False)\n",
    "    fig.update_yaxes(automargin=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers Definition\n",
    "Extract useful data from benchmarks outputs using regexps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_sysbench(raw_data, print_errors=True):\n",
    "    sys_res = []\n",
    "    cpu_tests = [(r\"events per second:\\s*([0-9.]*)\", \"cpu: events per second\", \"events/s\"),\n",
    "                (r\"avg:\\s*([0-9.]*)\", \"cpu: avg latency\", \"ms\")]\n",
    "    fileio_tests = [(r\"reads\\/s:\\s*([0-9.]*)\", \"fileio: fileop - read\", \"reads/s\"),\n",
    "                    (r\"writes\\/s:\\s*([0-9.]*)\", \"fileio: fileop - write\", \"writes/s\"),\n",
    "                    (r\"fsyncs\\/s:\\s*([0-9.]*)\", \"fileio: fileop - fsync\", \"fsyncs/s\"),\n",
    "                    (r\"read, MiB\\/s:\\s*([0-9.]*)\", \"fileio: throughput - read\", \"MiB/s\"),\n",
    "                    (r\"written, MiB\\/s:\\s*([0-9.]*)\", \"fileio: throughput - write\", \"MiB/s\"),\n",
    "                    (r\"avg:\\s*([0-9.]*)\", \"fileio: avg latency\", \"ms\")]\n",
    "    memory_tests = [(r\"([0-9.,]*) per second\", \"memory: totalops per second\", \"ops/s\"),\n",
    "                    (r\"([0-9.,]*) MiB\\/sec\", \"memory: speed\", \"MiB/s\"),\n",
    "                    (r\"avg:\\s*([0-9.]*)\", \"memory: avg latency\", \"ms\")]\n",
    "    threads_tests = [(r\"avg:\\s*([0-9.]*)\", \"threads: avg latency\", \"ms\")]\n",
    "    for benchmark in [(\"cpu\", cpu_tests), (\"fileio\", fileio_tests),\n",
    "                      (\"memory\", memory_tests), (\"threads\", threads_tests)]:\n",
    "        r_data = raw_data[benchmark[0]]\n",
    "        tests = benchmark[1]\n",
    "        for test in tests:\n",
    "            match = re.findall(test[0], r_data, re.MULTILINE)\n",
    "            res = float(match[0].replace(\",\", \".\"))\n",
    "            name = test[1]\n",
    "            unit = test[2]\n",
    "            sys_res.append({\"unit\": unit, \"value\": res, \"name\": name})\n",
    "    return sys_res\n",
    "\n",
    "def parser_simplecpu(raw_data, print_errors=True):\n",
    "    return [{\"unit\": \"s\", \"value\": raw_data, \"name\": \"mean duration\"}]\n",
    "\n",
    "def parser_download(raw_data, print_errors=True):\n",
    "    # extract size and time\n",
    "    regex = r\"Downloaded\\s([0-9]*)[a-z\\s]*([0-9.,]*)\"\n",
    "    match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "    size = float(match[0][0])/1000/1000\n",
    "    time = float(match[0][1].replace(\",\", \".\"))\n",
    "    return [{\"unit\": \"MB/s\", \"value\": round(size/time, 2), \"name\": \"speed\"}]\n",
    "\n",
    "def parser_dd(raw_data, print_errors=True):\n",
    "    # extract speed\n",
    "    regex = r\"s,\\s([0-9.,]*)\\s[a-zA-Z]*\\/s\"\n",
    "    match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "    speed = float(str(match[0]).replace(\",\", \".\"))\n",
    "    return [{\"unit\": \"MB/s\", \"value\": speed, \"name\": \"speed\"}]\n",
    "\n",
    "def parser_web_benchmark(raw_data, print_errors=True):\n",
    "    regex = r\"Requests/sec:\\s*([0-9.]*)\"\n",
    "    match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "    speed = float(str(match[0]).replace(\",\", \".\"))\n",
    "    return [{\"unit\": \"req/s\", \"value\": speed, \"name\": \"requests per second\"}]\n",
    "\n",
    "def parser_nench(raw_data, print_errors=True):\n",
    "    # cpu\n",
    "    try:\n",
    "        cpu_res = []\n",
    "        for cpu_test in [\"SHA256\", \"bzip2\", \"AES\"]:\n",
    "            regex = re.escape(cpu_test) + \".*\\n\\s*([0-9,.]*) ([a-z]*)\"\n",
    "            match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "            res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "            unit = str(match[0][1])\n",
    "            cpu_res.append({\"unit\": unit, \"value\": res, \"name\": \"cpu - \" + cpu_test})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - cpu: \" + str(e))\n",
    "    # ioping\n",
    "    try:\n",
    "        ioping_res = []\n",
    "        seekrate = (r\"min\\/avg\\/max\\/mdev = [0-9a-z.\\s]*\\/ ([0-9.]*) ([a-z]*)\", \"ioping: avg seek rate\")\n",
    "        srs = (r\"generated.*iops. ([0-9.]*) (.*)\", \"ioping: sequential read speed\")\n",
    "        for ioping in [seekrate, srs]:\n",
    "            regex = ioping[0]\n",
    "            match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "            res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "            unit = str(match[0][1])\n",
    "            ioping_res.append({\"unit\": unit, \"value\": res, \"name\": ioping[1]})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - ioping: \" + str(e))\n",
    "        pass\n",
    "    # dd\n",
    "    try:\n",
    "        dd_res = []\n",
    "        regex = r\"average:\\s*([0-9.]*) ([a-zA-Z\\/]*)$\"\n",
    "        match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "        res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "        unit = str(match[0][1])\n",
    "        dd_res.append({\"unit\": unit, \"value\": res, \"name\": \"dd: avg sequential write speed\"})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - dd: \" + str(e))\n",
    "    # speed test\n",
    "    try:\n",
    "        speed_res = []\n",
    "        for speed_test in [\"Cachefly CDN\", \"Leaseweb (NL)\", \"Softlayer DAL (US)\", \"Online.net (FR)\", \"OVH BHS (CA)\"]:\n",
    "            regex = re.escape(speed_test) + \":\\s*([0-9.]*) ([a-zA-Z\\/]*)\"\n",
    "            match = re.findall(regex, raw_data, re.MULTILINE)\n",
    "            res = float(str(match[0][0]).replace(\",\", \".\"))\n",
    "            unit = str(match[0][1])\n",
    "            speed_res.append({\"unit\": unit, \"value\": res, \"name\": \"net speed - \" + speed_test})\n",
    "    except Exception as e:\n",
    "        if print_errors:\n",
    "            print(\"error: nench - speed test: \" + str(e))\n",
    "    return cpu_res + ioping_res + dd_res + speed_res\n",
    "\n",
    "def parser_aibenchmark(raw_data, print_errors=True):\n",
    "    ai_res = []\n",
    "    if raw_data[\"ai_score\"]:\n",
    "        ai_res.append({\"unit\": \"score\", \"value\": raw_data[\"ai_score\"], \"name\": \"ai score\"})\n",
    "    if raw_data[\"inference_score\"]:\n",
    "        ai_res.append({\"unit\": \"score\", \"value\": raw_data[\"inference_score\"], \"name\": \"inference score\"})\n",
    "    if raw_data[\"training_score\"]:\n",
    "        ai_res.append({\"unit\": \"score\", \"value\": raw_data[\"training_score\"], \"name\": \"training score\"})\n",
    "    return ai_res\n",
    "\n",
    "def htr(unit):\n",
    "    if unit == \"s\" or unit == \"seconds\" or unit ==\"ms\" or unit == \"us\":\n",
    "        return \"LIB\"\n",
    "    return \"HIB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply parsers and extract data\n",
    "For each benchmark extract a list of values (a benchmark can provide multiple extracted values, such as sysbench):\n",
    "- extracted.name: is the name of the benchmark value extracted, e.g. cpu - SHA256\n",
    "- extracted.value: is the extracted value, e.g. 3.279\n",
    "- extracted.unit: extracted unit, e.g. seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark_extractors = {\"download\": parser_download,\n",
    "                        \"dd\": parser_dd,\n",
    "                        \"web-benchmark\": parser_web_benchmark,\n",
    "                        \"nench-benchmark\": parser_nench,\n",
    "                        \"sys-benchmark\": parser_sysbench,\n",
    "                        \"simple-cpu\": parser_simplecpu,\n",
    "                        \"ai-benchmark\": parser_aibenchmark}\n",
    "\n",
    "\n",
    "# get configuration\n",
    "with open(config_file, 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "if source == \"local\":\n",
    "    with open(source_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "elif source == \"remote\":\n",
    "    # get data from the database\n",
    "    url = cfg[\"bin_database_url\"] + \"/all\"\n",
    "    # print(\"getting data from: \" + url)\n",
    "    response = requests.get(url=url)\n",
    "    data = response.json()\n",
    "\n",
    "print(\"total VMs: %d\\n\\n\" % len(data[\"bins\"]))\n",
    "\n",
    "durations_all = []\n",
    "benchmarks_all = 0\n",
    "# iter vms\n",
    "for bin in data[\"bins\"]:\n",
    "    pars_err = 0\n",
    "    id = bin[\"id\"]\n",
    "    if id in ignore_vms_list:\n",
    "        continue\n",
    "    updated = bin[\"updated\"]\n",
    "    benchmarks = len(bin[\"values\"])\n",
    "    server = bin[\"values\"][0][\"server\"]\n",
    "    \n",
    "    print(id)\n",
    "    print(json.dumps(server, indent=2))\n",
    "    \n",
    "    # iters bench runs\n",
    "    durations = []\n",
    "    times = []\n",
    "    results = {}\n",
    "    for value in bin[\"values\"]:\n",
    "        durations_all.append(value[\"duration\"])\n",
    "        durations.append(value[\"duration\"])\n",
    "        time = datetime.datetime.strptime(value[\"time\"], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        times.append(time)\n",
    "        \n",
    "        # iter benchmarks\n",
    "        for benchmark in value[\"benchmarks\"]:\n",
    "            if benchmark[\"name\"] in benchmark_extractors.keys():\n",
    "                try:\n",
    "                    benchmark[\"extracted\"] = benchmark_extractors[benchmark[\"name\"]](benchmark[\"result\"][\"output\"], print_errors=False)\n",
    "                    benchmarks_all += 1\n",
    "                except Exception as e:\n",
    "                    print(\"Error extracting: \" + benchmark[\"name\"] + \" @ \" + str(time) + \"\\n\" + str(e))\n",
    "                    pars_err += 1\n",
    "    print(\"Parsing errors: \" + str(pars_err))\n",
    "        \n",
    "    print(\"total runs: %d\\nupdated: %s\\nduration (min, avg, max): %.2f m, %.2f m, %.2f m\"\n",
    "          % (benchmarks, updated, min(durations)/60, stat.mean(durations)/60, max(durations)/60))\n",
    "    \n",
    "    fig = go.Figure([go.Scatter(x=times, y=durations)])\n",
    "    fig.update_layout(\n",
    "        title=provider + \" / \" + cfg[\"vms\"][id.split(\"_\", 1)[1]][\"name\"],\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"time\",\n",
    "        yaxis_title=\"duration\",\n",
    "    )\n",
    "    fig.write_image(\"output/execution/img/\" + id + \".png\", scale=2)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.write_html(\"output/execution/html/\" + id + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.show()\n",
    "\n",
    "print(\"OVERALL benchmarks: %d\\nduration (min, avg, max): %.2f m, %.2f m, %.2f m\"\n",
    "          % (benchmarks_all, min(durations_all)/60, stat.mean(durations_all)/60, max(durations_all)/60))\n",
    "# print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "- use Pandas to analyze data\n",
    "- preparare the data for Pandas: build a list where each entry is a benchmark value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "tests_uni = {}\n",
    "# iter vms\n",
    "for bin in data[\"bins\"]:\n",
    "    id = bin[\"id\"]\n",
    "    if id in ignore_vms_list:\n",
    "        continue\n",
    "    updated = bin[\"updated\"]\n",
    "    server = bin[\"values\"][0][\"server\"]\n",
    "    \n",
    "    # iters bench run\n",
    "    for value in bin[\"values\"]:\n",
    "        duration = value[\"duration\"]\n",
    "        time = value[\"time\"]\n",
    "        \n",
    "        # iter benchmarks\n",
    "        for benchmark in value[\"benchmarks\"]:\n",
    "            if \"extracted\" not in benchmark:\n",
    "                break\n",
    "            extracted = benchmark[\"extracted\"]\n",
    "            \n",
    "            for ext in extracted:\n",
    "                test_id = hashlib.sha1(str(str(benchmark[\"name\"] + str(benchmark[\"setup\"]) + ext[\"name\"])).encode()).hexdigest()\n",
    "                row = {\"id\": cfg[\"vms\"][id.split(\"_\", 1)[1]][\"name\"],\n",
    "                       \"duration\": duration,\n",
    "                       \"time\": pd.to_datetime(time, format='%Y-%m-%d %H:%M:%S.%f'),\n",
    "                       \"value\": ext[\"value\"],\n",
    "                       \"test_id\": test_id}\n",
    "                \n",
    "                tests_uni[test_id] = {\"bench_name\": benchmark[\"name\"],\n",
    "                                      \"setup\": str(benchmark[\"setup\"]),\n",
    "                                      \"extracted_name\": ext[\"name\"],\n",
    "                                      \"extracted_unit\": ext[\"unit\"],\n",
    "                                      \"htr\": htr(ext[\"unit\"]) }\n",
    "                \n",
    "                rows.append(row)\n",
    "                \n",
    "# save tests\n",
    "pickle.dump(tests_uni, open(\"pickle/tests_uni.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(tests_uni, indent=2))\n",
    "print(\"\\nBenchmark list (\" + str(len(tests_uni)) + \"):\")\n",
    "for test in tests_uni:\n",
    "    print(tests_uni[test][\"bench_name\"], tests_uni[test][\"extracted_name\"], tests_uni[test][\"extracted_unit\"], tests_uni[test][\"htr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(rows))\n",
    "# normalize the data\n",
    "pd.set_option('display.max_rows', 20)\n",
    "df = pd.json_normalize(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the extracted values for each benchmark with all the machines in a single graph\n",
    "1. compute the mean for each benchmark for each machine grouping by: id, test_id, time\n",
    "2. reset_index() to obtain a list of tuples\n",
    "3. groupby bench_name, setup, extracted.name\n",
    "4. plot for each group, grouping by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 20)\n",
    "gb = df.groupby(['id', 'test_id', 'time']).agg(\n",
    "    {'value': ['mean']}).reset_index()\n",
    "gb.columns = gb.columns.droplevel(1)\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "gbb = gb.groupby(['test_id'])\n",
    "# save gbb\n",
    "pickle.dump(gbb, open(\"pickle/\" + provider + \".p\", \"wb\"))\n",
    "gbb.first() # print the first entry (can be many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Result Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(provider, bench_name, extracted_name, key):\n",
    "    return provider + \"_\" + get_valid_filename(bench_name + \"_\" + extracted_name) + \"_\" + str(key)[0:5]\n",
    "def get_trace_name(name):\n",
    "    if provider == \"aws\":\n",
    "        #return name.replace(\"-central\", \"\")\n",
    "        return name[:name.find(\" -\")]+\"-\"+name[-1:]\n",
    "    elif provider == \"azure\":\n",
    "        return name\n",
    "    elif provider == \"gcp\":\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, grp in gbb:\n",
    "    print(tests_uni[key])\n",
    "    print(\"\\n\")\n",
    "    fig = go.Figure()\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        times = grp2[\"time\"]\n",
    "        values = grp2[\"value\"]\n",
    "        fig.add_trace(go.Scatter(x=times, y=values, mode='lines', name=get_trace_name(key2)))\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        title=provider + \" / \" + tests_uni[key][\"bench_name\"] + \" - \" + tests_uni[key][\"extracted_name\"] + \" (\" + tests_uni[key][\"extracted_unit\"] + \")\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"time\",\n",
    "        yaxis_title=\"result\",\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(\"output/time/html/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.write_image(\"output/time/img/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Result Bar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, grp in gbb:\n",
    "    print(tests_uni[key])\n",
    "    print(\"\\n\")\n",
    "    fig = go.Figure()\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        print(\"%s - \\tAVG: %.2f, STD: %.2f, MIN: %.2f, MAX: %.2f\" % (key2, grp2.value.mean(), grp2.value.std(), grp2.value.min(), grp2.value.max()))\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=get_trace_name(key2),\n",
    "            x=[get_trace_name(key2)], y=[grp2.value.mean()],\n",
    "            error_y=dict(type='data', array=[grp2.value.std()])\n",
    "        ))\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        title=provider + \" / \" + tests_uni[key][\"bench_name\"] + \" - \" + tests_uni[key][\"extracted_name\"] + \" (\" + tests_uni[key][\"extracted_unit\"] + \")\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"instance\",\n",
    "        yaxis_title=\"result\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(\"output/bar/html/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.write_image(\"output/bar/img/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Result Delta Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, grp in gbb:\n",
    "    print(tests_uni[key])\n",
    "    print(\"\\n\")\n",
    "    fig = go.Figure()\n",
    "    gbid = grp.groupby(\"id\")\n",
    "    for key2, grp2 in gbid:\n",
    "        times = grp2[\"time\"]\n",
    "        values = [v/grp2.value.mean()*100 for v in grp2[\"value\"]]\n",
    "        fig.add_trace(go.Scatter(x=times, y=values, mode='lines', name=get_trace_name(key2)))\n",
    "        print(\"%s - \\tAVG: %.2f, STD: %.2f, MIN: %.2f, MAX: %.2f\" % (key2, stat.mean(values), stat.stdev(values), min(values), max(values)))\n",
    "    fig.update_layout(autosize=True)\n",
    "    fig.update_yaxes(automargin=True)\n",
    "    fig.update_layout(\n",
    "        title=provider + \" / \" + tests_uni[key][\"bench_name\"] + \" - \" + tests_uni[key][\"extracted_name\"] + \" (\" + tests_uni[key][\"extracted_unit\"] + \")\",\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"time\",\n",
    "        yaxis_title=\"Î”\",\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_html(\"output/delta/html/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".html\", include_plotlyjs=\"cdn\")\n",
    "    fig.write_image(\"output/delta/img/\" + get_filename(provider, tests_uni[key][\"bench_name\"], tests_uni[key][\"extracted_name\"], key) + \".png\", scale=2)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
